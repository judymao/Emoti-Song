{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMFDB Data - Preliminary\n",
    "\n",
    "Organize the data into labelled folders, separated by training, test, and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def org_IMFDB(raw_data_path):\n",
    "    \"\"\"\n",
    "    To clean the IMFDB data, we need to do the following:\n",
    "    \n",
    "    - remove Disgust and Surprise datasets\n",
    "    - split into each label\n",
    "    - split into training, test, and validation datasets\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    allowed_emotions = ['HAPPINESS', 'NEUTRAL', 'SADNESS', 'FEAR', 'ANGER']\n",
    "    emotion_mapping = {'HAPPINESS':'Happy', 'NEUTRAL':'Neutral', 'SADNESS':'Sad', 'FEAR':'Fear','ANGER':'Anger'}\n",
    "\n",
    "    # Organize images into: label/dataset folders, where label is an allowed emotion and dataset is train/validation/test\n",
    "\n",
    "    random.seed(50)\n",
    "\n",
    "    # First divide actors into train/validation/test\n",
    "    actors = os.listdir(IMFDB_raw_path)\n",
    "    random.shuffle(actors)\n",
    "\n",
    "    actor_dict = {}\n",
    "    train_actors = actors[:int(len(actors)*0.8)] # 80% test dataset\n",
    "    validation_actors = actors[int(len(actors)*0.8):int(len(actors)*0.9)] # 10% validation dataset\n",
    "    test_actors = actors[int(len(actors)*0.9):] # 10% validation dataset\n",
    "\n",
    "    for actor in train_actors:\n",
    "        actor_dict[actor] = 'training'\n",
    "\n",
    "    for actor in validation_actors:\n",
    "        actor_dict[actor] = 'validation'\n",
    "\n",
    "    for actor in test_actors:\n",
    "        actor_dict[actor] = 'test'\n",
    "\n",
    "    # Iterate through each actor\n",
    "    for actor in os.listdir(raw_data_path):\n",
    "        # Iterate through each movie\n",
    "        for movie in os.listdir(os.path.join(raw_data_path, actor)):\n",
    "            text_file = os.path.join(raw_data_path, actor, movie, movie+'.txt')\n",
    "            image_folder = os.path.join(raw_data_path, actor, movie, 'images')\n",
    "\n",
    "\n",
    "            try:\n",
    "                data = pd.read_csv(text_file, header=None, sep='\\t').rename({1:'image', 10:'emotion'}, axis=1)\n",
    "\n",
    "                i = 1 # Find the emotion column\n",
    "                while data.iloc[0,i+9] not in ['HAPPINESS', 'NEUTRAL', 'SADNESS', 'FEAR', 'ANGER', 'SURPRISE', 'DISGUST'] and i < len(data.columns)-1:\n",
    "                    data = data.rename({'image':i, i+1:'image','emotion':i+9,i+10:'emotion'}, axis=1)\n",
    "                    i+=1\n",
    "\n",
    "                emotion_dict = dict(zip(data.image, data.emotion))\n",
    "\n",
    "                for image_file in os.listdir(image_folder):\n",
    "                    if image_file in emotion_dict.keys() and emotion_dict[image_file] in allowed_emotions:\n",
    "                        # copy to new folder\n",
    "                        shutil.copy(os.path.join(image_folder,image_file), os.path.join(os.getcwd(), 'data_IMFDB',\n",
    "                                                                                        actor_dict[actor], \n",
    "                                                                                        emotion_mapping[emotion_dict[image_file]],\n",
    "                                                                                        image_file))\n",
    "            except:\n",
    "                print(data.head())\n",
    "                print(actor, movie, sys.exc_info()[0])\n",
    "    \n",
    "    print(\"Data successfully moved from raw files folder split by actor and movie to label folders split by training, validation, and testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMFDB Data - Cleaning\n",
    "\n",
    "Loading into datasets and applying cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacialDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, transform=None):\n",
    "        self.data = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.data.iloc[index, 1]\n",
    "        label = self.data.iloc[index, 0]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(50)\n",
    "def clean_IMFDB(raw_data_path):\n",
    "    \"\"\"\n",
    "    To clean the IMFDB data, we need to do the following:\n",
    "    \n",
    "    - resize datasets to 48x48 pixels\n",
    "    - convert to greyscale (1 channel)\n",
    "    - normalize the pixel values\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize(48), # Change dimensions to 48x48\n",
    "        transforms.CenterCrop(48), # Convert to square aspect\n",
    "        transforms.ToTensor() # Convert pixels to 0-1 range\n",
    "    ])\n",
    "\n",
    "    train_datasets = torchvision.datasets.ImageFolder(root=os.path.join(raw_data_path,'training'),\n",
    "                                                      transform=data_transforms)\n",
    "    train_list = []\n",
    "    for (data, label) in train_datasets:\n",
    "        train_list.append((label, data.numpy()))\n",
    "\n",
    "    train_df = pd.DataFrame(train_list)\n",
    "    train_dataset = FacialDataset(train_df, transform=transforms.ToTensor())\n",
    "    \n",
    "    val_datasets = torchvision.datasets.ImageFolder(root=os.path.join(raw_data_path,'validation'),\n",
    "                                                    transform=data_transforms)\n",
    "    val_list = []\n",
    "    for (data, label) in val_datasets:\n",
    "        val_list.append((label, data.numpy()))\n",
    "\n",
    "    val_df = pd.DataFrame(val_list)\n",
    "    val_dataset = FacialDataset(val_df, transform=transforms.ToTensor())\n",
    "    \n",
    "    test_datasets = torchvision.datasets.ImageFolder(root=os.path.join(raw_data_path,'test'),\n",
    "                                                     transform=data_transforms)\n",
    "    test_list = []\n",
    "    for (data, label) in test_datasets:\n",
    "        test_list.append((label, data.numpy()))\n",
    "\n",
    "    test_df = pd.DataFrame(test_list) \n",
    "    test_dataset = FacialDataset(test_df, transform=transforms.ToTensor())\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IMFDB_raw_path = os.path.join(os.getcwd(), \"raw_facial_data_IMFDB\")\n",
    "# org_IMFDB(IMFDB_raw_path)\n",
    "\n",
    "IMFDB_path = os.path.join(os.getcwd(), \"data_IMFDB\")\n",
    "IMFDB_train, IMFDB_val, IMFDB_test = clean_IMFDB(IMFDB_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of IMFDB training images:  16392\n",
      "Number of IMFDB validation images:  2147\n",
      "Number of IMFDB test images:  2170\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of IMFDB training images: \", len(IMFDB_train))\n",
    "print(\"Number of IMFDB validation images: \", len(IMFDB_val))\n",
    "print(\"Number of IMFDB test images: \", len(IMFDB_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "train_counts = dict(Counter(sample_tup[1] for sample_tup in IMFDB_train))\n",
    "val_counts = dict(Counter(sample_tup[1] for sample_tup in IMFDB_val))\n",
    "test_counts = dict(Counter(sample_tup[1] for sample_tup in IMFDB_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train images in each IMFDB class \t\t {0: 1975, 1: 471, 2: 5566, 3: 5859, 4: 2521}\n",
      "Number of validation images in each IMFDB class \t {0: 307, 1: 79, 2: 566, 3: 940, 4: 255}\n",
      "Number of test images in each IMFDB class \t\t {0: 213, 1: 17, 2: 651, 3: 855, 4: 434}\n"
     ]
    }
   ],
   "source": [
    "print('Number of train images in each IMFDB class \\t\\t', train_counts)\n",
    "print('Number of validation images in each IMFDB class \\t', val_counts)\n",
    "print('Number of test images in each IMFDB class \\t\\t', test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_str_to_array(row):\n",
    "    \"\"\"\n",
    "    Convert a string space-separated pixel values in row major order into a 2D numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert into 1D numpy array\n",
    "    arr_1D = np.fromstring(row, dtype=int, sep=\" \")\n",
    "    \n",
    "    # Convert into normalized 2D numpy array with 48 x 48 shape\n",
    "    arr_2D = np.reshape(arr_1D, (48, 48))/255\n",
    "    \n",
    "    img = Image.fromarray(arr_2D)\n",
    "    \n",
    "    return img\n",
    "\n",
    "    \n",
    "    \n",
    "def clean_Kaggle(raw_data_path):\n",
    "    \"\"\"\n",
    "    To clean the Kaggle data, we need to do the following:\n",
    "    \n",
    "    - remove Disgust and Surprise datasets\n",
    "    - normalize the pixels\n",
    "    - set training data to training data\n",
    "    - set public test data to validation data\n",
    "    - set private test data to test data\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    kaggle_raw = pd.read_csv(kaggle_raw_path)\n",
    "    \n",
    "    # Remove disgust (1) and surprise(5) data\n",
    "    kaggle_df = kaggle_raw.loc[(kaggle_raw.emotion != 1) & (kaggle_raw.emotion != 5)]\n",
    "    \n",
    "    emotion_labels = {0:0, 2:1, 3:2, 4:4, 6:3}\n",
    "    kaggle_df = kaggle_df.replace(emotion_labels)\n",
    "\n",
    "    # Convert pixel values from space-separted pixel values in row major order to normalized PIL images\n",
    "    kaggle_df['pil_imgs'] = kaggle_df[' pixels'].apply(lambda row: convert_str_to_array(row))\n",
    "    \n",
    "    \n",
    "    # Split into training, validation, and test datasets\n",
    "    train_df = kaggle_df[['emotion', 'pil_imgs']].loc[kaggle_df[' Usage']=='Training']\n",
    "    val_df = kaggle_df[['emotion', 'pil_imgs']].loc[kaggle_df[' Usage']=='PublicTest']\n",
    "    test_df = kaggle_df[['emotion', 'pil_imgs']].loc[kaggle_df[' Usage']=='PrivateTest']\n",
    "    \n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.ToTensor() # Convert to tensors\n",
    "    ])\n",
    "    \n",
    "    train_dataset = FacialDataset(train_df, transform=data_transforms)\n",
    "    val_dataset = FacialDataset(val_df, transform=data_transforms)\n",
    "    test_dataset = FacialDataset(test_df, transform=data_transforms)\n",
    "    \n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_raw_path = os.path.join(os.getcwd(), \"raw_facial_data_Kaggle\",\"icml_face_data.csv\")\n",
    "\n",
    "Kaggle_train, Kaggle_val, Kaggle_test = clean_Kaggle(kaggle_raw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Kaggle training images:  25102\n",
      "Number of Kaggle validation images:  3118\n",
      "Number of Kaggle test images:  3118\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Kaggle training images: \", len(Kaggle_train))\n",
    "print(\"Number of Kaggle validation images: \", len(Kaggle_val))\n",
    "print(\"Number of Kaggle test images: \", len(Kaggle_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Judy_Mao\\Anaconda3\\envs\\aps360\\lib\\site-packages\\torchvision\\transforms\\functional.py:74: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  img = torch.from_numpy(np.array(pic, np.float32, copy=False))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train images in each Kaggle class \t\t {0: 3995, 1: 4097, 4: 4830, 3: 4965, 2: 7215}\n",
      "Number of validation images in each Kaggle class \t {0: 467, 4: 653, 3: 607, 2: 895, 1: 496}\n",
      "Number of test images in each Kaggle class \t\t {0: 491, 3: 626, 4: 594, 1: 528, 2: 879}\n"
     ]
    }
   ],
   "source": [
    "train_counts = dict(Counter(sample_tup[1] for sample_tup in Kaggle_train))\n",
    "val_counts = dict(Counter(sample_tup[1] for sample_tup in Kaggle_val))\n",
    "test_counts = dict(Counter(sample_tup[1] for sample_tup in Kaggle_test))\n",
    "\n",
    "print('Number of train images in each Kaggle class \\t\\t', train_counts)\n",
    "print('Number of validation images in each Kaggle class \\t', val_counts)\n",
    "print('Number of test images in each Kaggle class \\t\\t', test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = [Kaggle_train, IMFDB_train]\n",
    "val_datasets = [Kaggle_val, IMFDB_val]\n",
    "test_datasets = [Kaggle_test, IMFDB_test]\n",
    "\n",
    "train_dataset = ConcatDataset(train_datasets)\n",
    "val_dataset = ConcatDataset(val_datasets)\n",
    "test_dataset = ConcatDataset(test_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images:  41494\n",
      "Number of validation images:  5265\n",
      "Number of test images:  5288\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training images: \", len(train_dataset))\n",
    "print(\"Number of validation images: \", len(val_dataset))\n",
    "print(\"Number of test images: \", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train images in each class \t\t {0: 5970, 1: 4568, 4: 7351, 3: 10824, 2: 12781}\n",
      "Number of validation images in each class \t {0: 774, 4: 908, 3: 1547, 2: 1461, 1: 575}\n",
      "Number of test images in each class \t\t {0: 704, 3: 1481, 4: 1028, 1: 545, 2: 1530}\n"
     ]
    }
   ],
   "source": [
    "train_counts = dict(Counter(sample_tup[1] for sample_tup in train_dataset))\n",
    "val_counts = dict(Counter(sample_tup[1] for sample_tup in val_dataset))\n",
    "test_counts = dict(Counter(sample_tup[1] for sample_tup in test_dataset))\n",
    "\n",
    "print('Number of train images in each class \\t\\t', train_counts)\n",
    "print('Number of validation images in each class \\t', val_counts)\n",
    "print('Number of test images in each class \\t\\t', test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_dataset, 'clean_facial_data/train_dataset')\n",
    "torch.save(val_dataset, 'clean_facial_data/val_dataset')\n",
    "torch.save(test_dataset, 'clean_facial_data/test_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torch.load('clean_facial_data/train_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(IMFDB_train, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
